
## ⚙️ Tech Stack

- **Python 3**
- **Selenium**: Automated browser scraping
- **Pandas, NumPy**: Data handling and transformation
- **GeoPandas, Folium, Plotly**: Geospatial mapping and analysis
- **Matplotlib, Seaborn**: Visualizations

## 🗺️ Data

The data is collected via custom web scraping scripts from [Fotocasa](https://www.fotocasa.es), one of the largest real estate websites in Spain. The analysis focuses specifically on listings within the Community of Madrid.

> **Note:** No personal or identifying information is scraped or stored. The dataset includes only public, anonymous listing data (e.g., price, size, location, number of rooms, etc.).

## 📌 Example Insights

- Distribution of price per square meter across Madrid's districts
- Identification of outliers and anomalous listings
- Interactive maps showing rental hotspots by neighborhood

## 📆 Timeline

This project is designed to be completed in ~30–40 hours over 4 weeks:
1. **Week 1**: Web scraping and raw data export
2. **Week 2**: Data cleaning and feature engineering
3. **Week 3**: Exploratory analysis and geospatial visualization
4. **Week 4**: Polishing code, documentation, and final deliverables

## ✍️ Author

**Diego Molina**  
_MSc. in Data Science & Economics_  

---

Feel free to use, fork, or extend this project for non-commercial purposes. Contributions and feedback are welcome!
