
## âš™ï¸ Tech Stack

- **Python 3**
- **Selenium**: Automated browser scraping
- **Pandas, NumPy**: Data handling and transformation
- **GeoPandas, Folium, Plotly**: Geospatial mapping and analysis
- **Matplotlib, Seaborn**: Visualizations

## ğŸ—ºï¸ Data

The data is collected via custom web scraping scripts from [Fotocasa](https://www.fotocasa.es), one of the largest real estate websites in Spain. The analysis focuses specifically on listings within the Community of Madrid.

> **Note:** No personal or identifying information is scraped or stored. The dataset includes only public, anonymous listing data (e.g., price, size, location, number of rooms, etc.).

## ğŸ“Œ Example Insights

- Distribution of price per square meter across Madrid's districts
- Identification of outliers and anomalous listings
- Interactive maps showing rental hotspots by neighborhood

## ğŸ“† Timeline

This project is designed to be completed in ~30â€“40 hours over 4 weeks:
1. **Week 1**: Web scraping and raw data export
2. **Week 2**: Data cleaning and feature engineering
3. **Week 3**: Exploratory analysis and geospatial visualization
4. **Week 4**: Polishing code, documentation, and final deliverables

## âœï¸ Author

**Diego Molina**  
_MSc. in Data Science & Economics_  

---

Feel free to use, fork, or extend this project for non-commercial purposes. Contributions and feedback are welcome!
